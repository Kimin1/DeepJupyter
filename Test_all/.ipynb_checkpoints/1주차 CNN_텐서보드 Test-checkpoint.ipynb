{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 획득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 데이터에서 검증 데이터 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val  = x_train[50000:60000]\n",
    "x_train = x_train[0:50000]\n",
    "y_val  = y_train[50000:60000]\n",
    "y_train = y_train[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train data has \" + str(x_train.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_train.shape[1])\n",
    "     + \"*\" + str(x_train.shape[2]) + \" image\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation data has \" + str(x_val.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_val.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample to show gray scale values\n",
    "print(x_train[0][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample to show labels for first train data to 10th train data\n",
    "print(y_train[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 구조 변경하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = np.reshape(x_train, (50000,28,28,1))\n",
    "x_val = np.reshape(x_val, (10000,28,28,1))\n",
    "x_test = np.reshape(x_test, (10000,28,28,1))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "x_val /= gray_scale\n",
    "x_test /= gray_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실제값을 원 핫 인코딩으로 변경하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN 텐서플로우로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파라미터(weight, bias) 초기값 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스트라이드, 패딩 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " conv1 레이어 설정  \n",
    " 활성화 함수 설정  \n",
    " 풀링레이어 적용  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('ConLayer1') as scope:\n",
    "    W_conv1 = weight_variable([5, 5, 1, 16])\n",
    "    b_conv1 = bias_variable([16])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "    \n",
    "    W_hist1 = tf.summary.histogram('weight1', W_conv1)\n",
    "    b_hist1 = tf.summary.histogram('bias1', b_conv1)\n",
    "    h_hist1 = tf.summary.histogram('hlayer1', h_conv1)\n",
    "\n",
    "    h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation map의 사이즈는 14 x 14가 되고 CON2 레이어 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('ConLayer2') as scope:\n",
    "    W_conv2 = weight_variable([5, 5, 16, 32])\n",
    "    b_conv2 = bias_variable([32])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "    W_hist2 = tf.summary.histogram('weight2', W_conv2)\n",
    "    b_hist2 = tf.summary.histogram('bias2', b_conv2)\n",
    "    h_hist2 = tf.summary.histogram('hlayer2', h_conv2)\n",
    "    \n",
    "    h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FC(Fully Connected Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONV를 통해 추출된 이미지의 특징들을 입력으로 받아 분류한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('FullyLayer1') as scope:    \n",
    "    W_fc1 = weight_variable([7 * 7 * 32, 128])\n",
    "    b_fc1 = bias_variable([128])\n",
    "\n",
    "    h_fc1_flat = tf.reshape(h_pool2, [-1, 7*7*32])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_fc1_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FC 영역에는 총 2개의 히든 레이어 존재, FC1은 128개, FC2는 10개의 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('FullyLayer2') as scope:      \n",
    "    W_fc2 = weight_variable([128, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크로스엔트로피를 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Cost') as scope:       \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y_conv))\n",
    "    cost_summ= tf.summary.scalar('cost',cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "옵티마이저를 사용해서 모델을 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도를 구하기 위해서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 및 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치를 사용하여 모델을 최적화. 미니배치는 총 500개의 학습데이터가 들어있고, 미니배치마다 파라미터를 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# train hyperparameters\n",
    "epoch_cnt = 3\n",
    "batch_size = 500\n",
    "iteration = len(x_train) // batch_size\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Writer logs and show graph \n",
    "    tf.set_random_seed(777)\n",
    "    merged_summary= tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('./CNN_logs2', sess.graph)\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.\n",
    "        start = 0; end = batch_size\n",
    "        feed_dict = {x:x_train[start: end],y_: y_train[start: end]}\n",
    "        \n",
    "        for step in tqdm_notebook(range(iteration)):\n",
    "          \n",
    "            if step%10 == 0:\n",
    "                summary = sess.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(summary, step)\n",
    "                train_acc = accuracy.eval(feed_dict=feed_dict)\n",
    "                \n",
    "                print(\"step \"+str(step)+ \": train acc: \"+str(train_acc))\n",
    "                \n",
    "            train_step.run(\n",
    "                feed_dict={\n",
    "                    x:x_train[start: end], \n",
    "                    y_: y_train[start: end]})\n",
    "            start += batch_size; end += batch_size    \n",
    "        \n",
    "        # Validate model\n",
    "        val_accuracy = accuracy.eval(feed_dict={x:x_val, y_: y_val})\n",
    "        print(\"validation accuracy: \"+str(val_accuracy))\n",
    "        \n",
    "    test_accuracy = accuracy.eval(feed_dict={x:x_test, y_: y_test}) \n",
    "    print(\"test accuracy: \"+str(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
